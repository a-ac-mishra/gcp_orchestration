
# TFX (Tensorflow Extended)

The dataset used to demonstrate the capability of tfx is chicago taxi trip dataset.
We can run the tfx components as pipeline or interactive context.
With interactive context we can get insights of the output produced by every components. 
This notebook covers both type of development. 

Spin up the user managed jupyter notebook on GCP from vertex AI and open up the **tfx_standalone.ipynb** and run the cells. 

0. Install the dependencies as mentioned in the notebook.

1. To start with the setup, assgined the following variables
    - `_tfx_root` - project directory
    - `_pipeline_root` - pipeline local or GCS path where artifacts will be stored which will be generated by the pipeline
    - `_metadata_db_root`
    - `_log_root` - path where log file will be generated
    - `_model_root` - model artifact path
    - `_serving_model_dir` - serving model directory
    - `_data_filepath` - data file path. e.g. train.csv etc
    - `_data_root` - data root path

2. The `CsvExampleGen` component which read the data from the path **_data_root**. This component produces two artifacts, training examples and evaluation examples.

3. The `StatisticsGen` component **computes statistics** over your dataset for data analysis, as well as for use in downstream components. `StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`.

4. The `SchemaGen` component generates a schema based on your data statistics( outputs of StatisticsGen ). (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library. `SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default.

5. The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library. `ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`.

6. Now, `TensorFlow Transform`, `Transform` component will transforms your data into features, and in the next step, these features will be used to train the ML model.

7. The `Trainer` component will train a model that you define in TensorFlow. Default Trainer support Estimator API, to use Keras API, you need to specify [Generic Trainer](https://github.com/tensorflow/community/blob/master/rfcs/20200117-tfx-generic-trainer.md) by setup `custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor)` in Trainer's contructor. `Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.

8. The `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`.

